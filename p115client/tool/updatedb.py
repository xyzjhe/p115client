#!/usr/bin/env python3
# encoding: utf-8

__author__ = "ChenyangGao <https://chenyanggao.github.io>"
__all__ = [
    "updatedb_initdb", "updatedb", "updatedb_life_iter", 
    "updatedb_history_iter", "P115QueryDB", 
]
__doc__ = "è¿™ä¸ªæ¨¡å—æä¾›äº†ä¸€äº›å’Œæ›´æ–°æ•°æ®åº“æœ‰å…³çš„å‡½æ•°"

from collections.abc import (
    AsyncIterator, Callable, Coroutine, Iterable, Iterator, Sequence, 
)
from csv import writer
from itertools import batched
from math import inf
from ntpath import normpath
from os import PathLike
from os.path import expanduser
from sqlite3 import connect, Connection, Cursor
from time import time
from typing import overload, Any, Literal
from warnings import warn

from asynctools import ensure_async
from errno2 import errno
from iterutils import (
    bfs_gen, chunked, foreach, run_gen_step, run_gen_step_iter, 
    with_iter_next, Yield, 
)
from orjson import dumps, loads
from p115client import P115Client, P115Warning
from p115pickcode import to_id
from posixpatht import path_is_dir_form, escape, splits
from sqlitetools import execute, find, query, transact, upsert_items, AutoCloseConnection

from .attr import get_ancestors_to_cid
from .iterdir import iter_nodes_using_event, traverse_tree
from .life import iter_life_behavior_list
from .history import iter_history_list


def updatedb_initdb(con: Connection | Cursor, /) -> Cursor:
    """åˆå§‹åŒ–æ•°æ®åº“ï¼Œç„¶åè¿”å›æ¸¸æ ‡
    """
    sql = """\
-- ä¿®æ”¹æ—¥å¿—æ¨¡å¼ä¸º WAL (Write Ahead Log)
PRAGMA journal_mode = WAL;

-- data è¡¨ï¼Œç”¨æ¥ä¿å­˜æ•°æ®
CREATE TABLE IF NOT EXISTS data (
    id INTEGER NOT NULL PRIMARY KEY,      -- ä¸»é”®
    parent_id INTEGER NOT NULL DEFAULT 0, -- ä¸Šçº§ç›®å½•çš„ id
    name TEXT NOT NULL,                   -- åå­—
    sha1 TEXT NOT NULL DEFAULT '',        -- æ–‡ä»¶çš„ sha1 æ•£åˆ—å€¼
    size INTEGER NOT NULL DEFAULT 0,      -- æ–‡ä»¶å¤§å°
    pickcode TEXT NOT NULL DEFAULT '',    -- æå–ç ï¼Œä¸‹è½½ç­‰æ“ä½œæ—¶éœ€è¦ç”¨åˆ°
    is_dir INTEGER NOT NULL DEFAULT 1 CHECK(is_dir IN (0, 1)), -- æ˜¯å¦ç›®å½•
    is_alive INTEGER NOT NULL DEFAULT 1 CHECK(is_alive IN (0, 1)), -- æ˜¯å¦å­˜æ´»ï¼ˆå­˜æ´»å³æ˜¯ä¸æ˜¯åˆ é™¤çŠ¶æ€ï¼‰
    extra BLOB DEFAULT NULL,              -- é¢å¤–çš„æ•°æ®
    created_at TIMESTAMP DEFAULT (unixepoch('subsec')), -- åˆ›å»ºæ—¶é—´
    updated_at TIMESTAMP DEFAULT (CAST(STRFTIME('%s', 'now') AS INTEGER))  -- æ›´æ–°æ—¶é—´
);

-- life è¡¨ï¼Œç”¨æ¥ä¿å­˜æ“ä½œäº‹ä»¶
CREATE TABLE IF NOT EXISTS life (
    id INTEGER NOT NULL PRIMARY KEY, -- æ–‡ä»¶æˆ–ç›®å½•çš„ id
    data JSON NOT NULL, -- æ•°æ®
    created_at TIMESTAMP DEFAULT (unixepoch('subsec')) -- åˆ›å»ºæ—¶é—´
);

-- history è¡¨ï¼Œç”¨æ¥ä¿å­˜å†å²è®°å½•
CREATE TABLE IF NOT EXISTS history (
    id INTEGER NOT NULL PRIMARY KEY, -- æ–‡ä»¶æˆ–ç›®å½•çš„ id
    data JSON NOT NULL, -- æ•°æ®
    created_at TIMESTAMP DEFAULT (unixepoch('subsec')) -- åˆ›å»ºæ—¶é—´
);

-- ç´¢å¼•
CREATE INDEX IF NOT EXISTS idx_data_pid ON data(parent_id);
CREATE INDEX IF NOT EXISTS idx_data_utime ON data(updated_at);

-- data è¡¨çš„è®°å½•å‘ç”Ÿæ›´æ–°ï¼Œè‡ªåŠ¨æ›´æ–°å®ƒçš„æ›´æ–°æ—¶é—´
CREATE TRIGGER IF NOT EXISTS trg_data_update
AFTER UPDATE ON data
FOR EACH ROW
BEGIN
    SELECT CASE
        WHEN NEW.updated_at < OLD.updated_at THEN RAISE(IGNORE)
    END;
    UPDATE data SET updated_at = CAST(STRFTIME('%s', 'now') AS INTEGER) WHERE id = NEW.id AND NEW.updated_at = OLD.updated_at;
END;

-- fs_event è¡¨ï¼Œç”¨æ¥ä¿å­˜æ–‡ä»¶ç³»ç»Ÿå˜æ›´ï¼ˆç”± data è¡¨è§¦å‘ï¼‰
CREATE TABLE IF NOT EXISTS fs_event (
    id INTEGER PRIMARY KEY AUTOINCREMENT, -- äº‹ä»¶ id
    event TEXT NOT NULL,                  -- äº‹ä»¶ç±»å‹ï¼šaddï¼ˆå¢ï¼‰ã€removeï¼ˆåˆ ï¼‰ã€renameï¼ˆæ”¹åï¼‰ã€moveï¼ˆç§»åŠ¨ï¼‰
    file_id INTEGER NOT NULL,             -- æ–‡ä»¶æˆ–ç›®å½•çš„ idï¼Œæ­¤ id å¿…åœ¨ `data` è¡¨ä¸­
    pid0 INTEGER NOT NULL DEFAULT -1,     -- å˜æ›´å‰ä¸Šçº§ç›®å½•çš„ id
    pid1 INTEGER NOT NULL DEFAULT -1,     -- å˜æ›´åä¸Šçº§ç›®å½•çš„ id
    name0 TEXT NOT NULL DEFAULT '',       -- å˜æ›´å‰çš„åå­—
    name1 TEXT NOT NULL DEFAULT '',       -- å˜æ›´åçš„åå­—
    created_at TIMESTAMP DEFAULT (unixepoch('subsec')) -- åˆ›å»ºæ—¶é—´
);

-- data è¡¨å‘ç”Ÿæ’å…¥
CREATE TRIGGER IF NOT EXISTS trg_data_insert
AFTER INSERT ON data
FOR EACH ROW
BEGIN
    INSERT INTO fs_event(event, file_id, pid1, name1) VALUES (
        'add', NEW.id, NEW.parent_id, NEW.name
    );
END;

-- data è¡¨å‘ç”Ÿè¿˜åŸ
CREATE TRIGGER IF NOT EXISTS trg_data_revoke
AFTER UPDATE ON data
FOR EACH ROW WHEN (NOT OLD.is_alive AND NEW.is_alive)
BEGIN
    INSERT INTO fs_event(event, file_id, pid1, name1) VALUES (
        'add', NEW.id, NEW.parent_id, NEW.name
    );
END;

-- data è¡¨å‘ç”Ÿç§»é™¤
CREATE TRIGGER IF NOT EXISTS trg_data_remove
AFTER UPDATE ON data
FOR EACH ROW WHEN (OLD.is_alive AND NOT NEW.is_alive)
BEGIN
    INSERT INTO fs_event(event, file_id, pid0, name0) VALUES (
        'remove', OLD.id, OLD.parent_id, OLD.name
    );
END;

-- data è¡¨å‘ç”Ÿæ”¹åæˆ–ç§»åŠ¨
CREATE TRIGGER IF NOT EXISTS trg_data_change
AFTER UPDATE ON data
FOR EACH ROW WHEN (OLD.is_alive AND NEW.is_alive)
BEGIN
    INSERT INTO fs_event(event, file_id, pid0, pid1, name0, name1)
    SELECT
        'move', OLD.id, OLD.parent_id, NEW.parent_id, OLD.name, OLD.name
    WHERE OLD.parent_id != NEW.parent_id;

    INSERT INTO fs_event(event, file_id, pid0, pid1, name0, name1) 
    SELECT * FROM (
        SELECT
            'rename', NEW.id, NEW.parent_id, NEW.parent_id, OLD.name, NEW.name
        WHERE OLD.name != NEW.name
    );
END;"""
    return con.executescript(sql)


def wrap_async(func, async_: bool = False, /, threaded: bool = False):
    if async_:
        return ensure_async(func, threaded=threaded)
    else:
        return func


def _init_client(
    client: str | PathLike | P115Client, 
    dbfile: None | str | PathLike | Connection | Cursor = None, 
) -> tuple[P115Client, Connection | Cursor]:
    if isinstance(client, (str, PathLike)):
        client = P115Client(client, check_for_relogin=True)
    if client.login_app() in ("web", "desktop", "harmony"):
        warn(
            f'app within ("web", "desktop", "harmony") is not recommended, it will be replaced by "apple_tv" cookies', 
            category=P115Warning, 
        )
        client.login_another_app("apple_tv", replace=True)
    if not dbfile:
        dbfile = f"p115db-{client.user_id}.db"
    if isinstance(dbfile, (Connection, Cursor)):
        con = dbfile
    else:
        con = connect(
            dbfile, 
            check_same_thread=False, 
            factory=AutoCloseConnection, 
            timeout=inf, 
            uri=isinstance(dbfile, str) and dbfile.startswith("file:"), 
        )
        updatedb_initdb(con)
    return client, con


def has_id(con: Connection | Cursor, id: int, /) -> int:
    sql = "SELECT 1 FROM data WHERE id = ? AND is_alive"
    return find(con, sql, (id,), default=0)


def event_normalize_attr(event: dict, /) -> dict:
    sha1 = event["sha1"]
    return {
        "id": int(event["file_id"]), 
        "parent_id": int(event["parent_id"]), 
        "name": event["file_name"], 
        "sha1": sha1, 
        "size": int(event.get("file_size") or 0), 
        "pickcode": event["pick_code"], 
        "is_dir": not sha1, 
        "is_alive": event["type"] != 22, 
        "updated_at": int(event["create_time"]), 
    }


@overload
def load_missing_ancestors(
    client: P115Client, 
    con: Connection | Cursor, 
    attrs: list[dict], 
    cooldown: float = 0.2, 
    app: str = "android", 
    *, 
    async_: Literal[False] = False, 
    **request_kwargs, 
) -> list[dict]:
    ...
@overload
def load_missing_ancestors(
    client: P115Client, 
    con: Connection | Cursor, 
    attrs: list[dict], 
    cooldown: float = 0.2, 
    app: str = "android", 
    *, 
    async_: Literal[True], 
    **request_kwargs, 
) -> Coroutine[Any, Any, list[dict]]:
    ...
def load_missing_ancestors(
    client: P115Client, 
    con: Connection | Cursor, 
    attrs: list[dict], 
    cooldown: float = 0.2, 
    app: str = "android", 
    *, 
    async_: Literal[False, True] = False, 
    **request_kwargs, 
) -> list[dict] | Coroutine[Any, Any, list[dict]]:
    def gen_step():
        seen_ids: set[int] = {a["id"] for a in attrs}
        ancestors: list[dict] = []
        add_to_seen = seen_ids.add
        add_ancestor = ancestors.append
        def add(attr: dict, /):
            add_to_seen(attr["id"])
            add_ancestor(attr)
        while pids := [
            pid for a in attrs 
            if (pid := a["parent_id"]) and not (pid in seen_ids or has_id(con, pid))
        ]:
            yield foreach(
                add, 
                iter_nodes_using_event(
                    client, 
                    pids, 
                    type="doc", 
                    normalize_attr=event_normalize_attr, 
                    id_to_dirnode=..., 
                    cooldown=cooldown, 
                    app=app, 
                    async_=async_, 
                    **request_kwargs, 
                ), 
            )
        return ancestors
    return run_gen_step(gen_step, async_)


@overload
def updatedb(
    client: str | PathLike | P115Client, 
    dbfile: None | str | PathLike | Connection | Cursor = None, 
    cid: int | str = 0, 
    app: str = "android", 
    *, 
    async_: Literal[False] = False, 
    **request_kwargs, 
) -> int:
    ...
@overload
def updatedb(
    client: str | PathLike | P115Client, 
    dbfile: None | str | PathLike | Connection | Cursor = None, 
    cid: int | str = 0, 
    app: str = "android", 
    *, 
    async_: Literal[True], 
    **request_kwargs, 
) -> Coroutine[Any, Any, int]:
    ...
def updatedb(
    client: str | PathLike | P115Client, 
    dbfile: None | str | PathLike | Connection | Cursor = None, 
    cid: int | str = 0, 
    app: str = "android", 
    *, 
    async_: Literal[False, True] = False, 
    **request_kwargs, 
) -> int | Coroutine[Any, Any, int]:
    """å¯¹æŸä¸ªç›®å½•æ‰§è¡Œä¸€æ¬¡å…¨é‡æ‹‰å–ï¼Œä»¥æ›´æ–° SQLite æ•°æ®

    :param client: 115 ç½‘ç›˜å®¢æˆ·ç«¯å¯¹è±¡
    :param dbfile: æ•°æ®åº“æ–‡ä»¶è·¯å¾„ï¼Œå¦‚æœä¸º Noneï¼Œåˆ™è‡ªåŠ¨ç¡®å®š
    :param cid: ç›®å½•çš„ id æˆ– pickcode
    :param app: ä½¿ç”¨æŒ‡å®š appï¼ˆè®¾å¤‡ï¼‰çš„æ¥å£
    :param async_: æ˜¯å¦å¼‚æ­¥
    :param request_kwargs: å…¶å®ƒè¯·æ±‚å‚æ•°

    :return: è¿”å›æ€»å…±å½±å“åˆ°æ•°æ®è¡Œæ•°ï¼Œå³æ‰€æœ‰ DML SQL æ‰§è¡Œåï¼Œæ¸¸æ ‡çš„ ``.rowcount`` ç´¯åŠ 
    """
    client, con = _init_client(client, dbfile)
    upsert = wrap_async(upsert_items, async_, threaded=True)
    cid = to_id(cid)
    def gen_step():
        start_t = int(time())
        total = 0
        if cid and not has_id(con, cid):
            ancestors = yield get_ancestors_to_cid(
                client, 
                cid, 
                id_to_dirnode=..., 
                app=app, 
                async_=async_, 
                **request_kwargs, 
            )
            if ancestors:
                if ancestors[0]["id"] == 0:
                    ancestors = ancestors[1:]
                if ancestors:
                    total += (yield upsert(con, ancestors, {"is_alive": 1}, commit=True)).rowcount
        with with_iter_next(chunked(
            traverse_tree(
                client, 
                cid, 
                id_to_dirnode=..., 
                app=app, 
                async_=async_, 
                **request_kwargs, 
            ), 
            1000, 
        )) as get_next:
            while True:
                batch = yield get_next()
                total += (yield upsert(con, batch, {"is_alive": 1}, commit=True)).rowcount
        if cid:
            clean_sql = f"""\
UPDATE data SET is_alive = 0 WHERE id in (
    WITH ids(id) AS (
        SELECT id FROM data WHERE parent_id = {cid} AND is_alive AND updated_at < :start_t
        UNION ALL
        SELECT data.id FROM ids JOIN data ON (ids.id = data.parent_id) WHERE is_alive AND updated_at < :start_t
    )
    SELECT id FROM ids
);"""
        else:
            clean_sql = "UPDATE data SET is_alive = 0 WHERE is_alive AND updated_at < :start_t"
        total += (yield wrap_async(execute, async_, threaded=True)(
            con, 
            clean_sql, 
            {"start_t": start_t}, 
            commit=True
        )).rowcount
        return total
    return run_gen_step(gen_step, async_)


@overload
def updatedb_life_iter(
    client: str | PathLike | P115Client, 
    dbfile: None | str | PathLike | Connection | Cursor = None, 
    from_id: int = -1, 
    from_time: float = 0, 
    cooldown: float = 0.2, 
    app: str = "android", 
    *, 
    async_: Literal[False] = False, 
    **request_kwargs, 
) -> Iterator[list[dict]]:
    ...
@overload
def updatedb_life_iter(
    client: str | PathLike | P115Client, 
    dbfile: None | str | PathLike | Connection | Cursor = None, 
    from_id: int = -1, 
    from_time: float = 0, 
    cooldown: float = 0.2, 
    app: str = "android", 
    *, 
    async_: Literal[True], 
    **request_kwargs, 
) -> AsyncIterator[list[dict]]:
    ...
def updatedb_life_iter(
    client: str | PathLike | P115Client, 
    dbfile: None | str | PathLike | Connection | Cursor = None, 
    from_id: int = -1, 
    from_time: float = 0, 
    cooldown: float = 0.2, 
    app: str = "android", 
    *, 
    async_: Literal[False, True] = False, 
    **request_kwargs, 
) -> Iterator[list[dict]] | AsyncIterator[list[dict]]:
    """æŒç»­é‡‡é›† 115 ç”Ÿæ´»æ—¥å¿—ï¼Œä»¥æ›´æ–° SQLite æ•°æ®åº“

    .. note::
        å½“ ``from_id < 0`` æ—¶ï¼Œä¼šä»æ•°æ®åº“è·å–æœ€å¤§ id ä½œä¸º ``from_id``ï¼Œè·å–ä¸åˆ°æ—¶è®¾ä¸º 0ã€‚
        å½“ ``from_id != 0`` æ—¶ï¼Œå¦‚æœ from_time ä¸º 0ï¼Œåˆ™è‡ªåŠ¨é‡è®¾ä¸º -1ã€‚

    :param client: 115 ç½‘ç›˜å®¢æˆ·ç«¯å¯¹è±¡
    :param dbfile: æ•°æ®åº“æ–‡ä»¶è·¯å¾„ï¼Œå¦‚æœä¸º Noneï¼Œåˆ™è‡ªåŠ¨ç¡®å®š
    :param from_id: å¼€å§‹çš„äº‹ä»¶ id ï¼ˆä¸å«ï¼‰ï¼Œè‹¥ < 0 åˆ™æ˜¯ä»æ•°æ®åº“è·å–æœ€å¤§ id
    :param from_time: å¼€å§‹æ—¶é—´ï¼ˆå«ï¼‰ï¼Œè‹¥ä¸º 0 åˆ™ä»å½“å‰æ—¶é—´å¼€å§‹ï¼Œè‹¥ < 0 åˆ™ä»æœ€æ—©å¼€å§‹
    :param cooldown: å†·å´æ—¶é—´ï¼Œå¤§äº 0 æ—¶ï¼Œä¸¤æ¬¡æ¥å£è°ƒç”¨ä¹‹é—´è‡³å°‘é—´éš”è¿™ä¹ˆå¤šç§’
    :param app: ä½¿ç”¨æŒ‡å®š appï¼ˆè®¾å¤‡ï¼‰çš„æ¥å£
    :param async_: æ˜¯å¦å¼‚æ­¥
    :param request_kwargs: å…¶å®ƒè¯·æ±‚å‚æ•°

    :return: è¿­ä»£å™¨ï¼Œæ¯æ¬¡äº§ç”Ÿä¸€æ‰¹äº‹ä»¶ï¼ˆä»å½“å‰åˆ°ä¸Šæ¬¡æˆªæ­¢ï¼‰

    .. code::

        from time import sleep
        from p115client import P115Client
        from p115client.tool import updatedb_life_iter

        client = P115Client.from_path()

        for event_list in updatedb_life_iter(client):
            if event_list:
                print("é‡‡é›†åˆ°æ“ä½œäº‹ä»¶åˆ—è¡¨:", event_list)
            else:
                sleep(1)
    """
    client, con = _init_client(client, dbfile)
    def gen_step():
        nonlocal from_id
        if from_id < 0:
            from_id = yield wrap_async(find, async_, threaded=True)(
                con, 
                "SELECT MAX(id) FROM life", 
                default=0, 
            )
        with with_iter_next(iter_life_behavior_list(
            client, 
            from_id=from_id, 
            from_time=from_time, 
            ignore_types=(10,), 
            cooldown=cooldown,         
            app=app, 
            async_=async_, 
            **request_kwargs, 
        )) as get_next:
            while True:
                event_list = yield get_next()
                event_list.reverse()
                if attrs := list(map(event_normalize_attr, event_list)):
                    if news := [a for a in attrs if a["is_alive"]]:
                        attrs.extend((yield load_missing_ancestors(
                            client, 
                            con, 
                            news, 
                            cooldown=cooldown, 
                            app=app, 
                            async_=async_, 
                            **request_kwargs, 
                        )))
                    yield wrap_async(upsert_items, async_, threaded=True)(
                        con, attrs, commit=True)
                if event_list:
                    yield wrap_async(execute, async_, threaded=True)(
                        con, 
                        "INSERT OR IGNORE INTO life(id, data) VALUES (?, ?)", 
                        [(int(event["id"]), dumps(event)) for event in event_list], 
                        commit=True, 
                    )
                yield Yield(event_list)
    return run_gen_step_iter(gen_step, async_)


@overload
def updatedb_history_iter(
    client: str | PathLike | P115Client, 
    dbfile: None | str | PathLike | Connection | Cursor = None, 
    from_id: int = -1, 
    from_time: float = 0, 
    cooldown: float = 0.2, 
    app: str = "android", 
    *, 
    async_: Literal[False] = False, 
    **request_kwargs, 
) -> Iterator[list[dict]]:
    ...
@overload
def updatedb_history_iter(
    client: str | PathLike | P115Client, 
    dbfile: None | str | PathLike | Connection | Cursor = None, 
    from_id: int = -1, 
    from_time: float = 0, 
    cooldown: float = 0.2, 
    app: str = "android", 
    *, 
    async_: Literal[True], 
    **request_kwargs, 
) -> AsyncIterator[list[dict]]:
    ...
def updatedb_history_iter(
    client: str | PathLike | P115Client, 
    dbfile: None | str | PathLike | Connection | Cursor = None, 
    from_id: int = -1, 
    from_time: float = 0, 
    cooldown: float = 0.2, 
    app: str = "android", 
    *, 
    async_: Literal[False, True] = False, 
    **request_kwargs, 
) -> Iterator[list[dict]] | AsyncIterator[list[dict]]:
    """æŒç»­é‡‡é›† 115 å†å²è®°å½•ï¼Œä»¥æ›´æ–° SQLite æ•°æ®åº“

    .. note::
        å½“ ``from_id < 0`` æ—¶ï¼Œä¼šä»æ•°æ®åº“è·å–æœ€å¤§ id ä½œä¸º ``from_id``ï¼Œè·å–ä¸åˆ°æ—¶è®¾ä¸º 0ã€‚
        å½“ ``from_id != 0`` æ—¶ï¼Œå¦‚æœ from_time ä¸º 0ï¼Œåˆ™è‡ªåŠ¨é‡è®¾ä¸º -1ã€‚

    :param client: 115 ç½‘ç›˜å®¢æˆ·ç«¯å¯¹è±¡
    :param dbfile: æ•°æ®åº“æ–‡ä»¶è·¯å¾„ï¼Œå¦‚æœä¸º Noneï¼Œåˆ™è‡ªåŠ¨ç¡®å®š
    :param from_id: å¼€å§‹çš„äº‹ä»¶ id ï¼ˆä¸å«ï¼‰ï¼Œè‹¥ < 0 åˆ™æ˜¯ä»æ•°æ®åº“è·å–æœ€å¤§ id
    :param from_time: å¼€å§‹æ—¶é—´ï¼ˆå«ï¼‰ï¼Œè‹¥ä¸º 0 åˆ™ä»å½“å‰æ—¶é—´å¼€å§‹ï¼Œè‹¥ < 0 åˆ™ä»æœ€æ—©å¼€å§‹
    :param cooldown: å†·å´æ—¶é—´ï¼Œå¤§äº 0 æ—¶ï¼Œä¸¤æ¬¡æ¥å£è°ƒç”¨ä¹‹é—´è‡³å°‘é—´éš”è¿™ä¹ˆå¤šç§’
    :param app: ä½¿ç”¨æŒ‡å®š appï¼ˆè®¾å¤‡ï¼‰çš„æ¥å£
    :param async_: æ˜¯å¦å¼‚æ­¥
    :param request_kwargs: å…¶å®ƒè¯·æ±‚å‚æ•°

    :return: è¿­ä»£å™¨ï¼Œæ¯æ¬¡äº§ç”Ÿä¸€æ‰¹äº‹ä»¶ï¼ˆä»å½“å‰åˆ°ä¸Šæ¬¡æˆªæ­¢ï¼‰

    .. code::

        from time import sleep
        from p115client import P115Client
        from p115client.tool import updatedb_history_iter

        client = P115Client.from_path()

        for event_list in updatedb_history_iter(client):
            if event_list:
                print("é‡‡é›†åˆ°å†å²è®°å½•åˆ—è¡¨:", event_list)
            else:
                sleep(1)
    """
    client, con = _init_client(client, dbfile)
    def gen_step():
        nonlocal from_id
        if from_id < 0:
            from_id = yield wrap_async(find, async_, threaded=True)(
                con, 
                "SELECT MAX(id) FROM history", 
                default=0, 
            )
        with with_iter_next(iter_history_list(
            client, 
            from_id=from_id, 
            from_time=from_time, 
            ignore_types=(), 
            cooldown=cooldown,         
            app=app, 
            async_=async_, 
            **request_kwargs, 
        )) as get_next:
            while True:
                event_list = yield get_next()
                event_list.reverse()
                if attrs := list(map(event_normalize_attr, event_list)):
                    if news := [a for a in attrs if a["is_alive"]]:
                        attrs.extend((yield load_missing_ancestors(
                            client, 
                            con, 
                            news, 
                            cooldown=cooldown, 
                            app=app, 
                            async_=async_, 
                            **request_kwargs, 
                        )))
                    yield wrap_async(upsert_items, async_, threaded=True)(
                        con, attrs, commit=True)
                if event_list:
                    yield wrap_async(execute, async_, threaded=True)(
                        con, 
                        "INSERT OR IGNORE INTO history(id, data) VALUES (?, ?)", 
                        [(int(event["id"]), dumps(event)) for event in event_list], 
                        commit=True, 
                    )
                yield Yield(event_list)
    return run_gen_step_iter(gen_step, async_)


class P115QueryDB:
    """å°è£…äº†ä¸€äº›å¸¸ç”¨çš„æ•°æ®åº“æŸ¥è¯¢æ–¹æ³•ï¼Œé’ˆå¯¹ updatedb äº§ç”Ÿçš„ SQLite æ•°æ®åº“

    .. note::
        é»˜è®¤æƒ…å†µä¸‹ï¼Œåªæœ‰ "id"ã€"parent_id"ã€"updated_at" æœ‰ç´¢å¼•ï¼Œæ‰€ä»¥è¯·è‡ªè¡Œæ·»åŠ å…¶å®ƒéœ€è¦çš„ç´¢å¼•
    """
    __slots__ = "con",

    def __init__(
        self, 
        /, 
        con: str | PathLike | Connection | Cursor, 
    ):
        if not isinstance(con, (Connection, Cursor)):
            con = connect(
                con, 
                check_same_thread=False, 
                factory=AutoCloseConnection, 
                timeout=inf, 
                uri=isinstance(con, str) and con.startswith("file:"), 
            )
        self.con = con

    def get_ancestors(
        self, 
        id: int = 0, 
        /, 
    ) -> list[dict]:
        """è·å–æŸä¸ªæ–‡ä»¶æˆ–ç›®å½•çš„ç¥–å…ˆèŠ‚ç‚¹ä¿¡æ¯ï¼ŒåŒ…æ‹¬ "id"ã€"parent_id" å’Œ "name" å­—æ®µ

        :param self: P115QueryDB å®ä¾‹æˆ–è€…æ•°æ®åº“è¿æ¥æˆ–æ¸¸æ ‡
        :param id: å½“å‰èŠ‚ç‚¹çš„ id

        :return: å½“å‰èŠ‚ç‚¹çš„ç¥–å…ˆèŠ‚ç‚¹åˆ—è¡¨ï¼Œä»æ ¹ç›®å½•å¼€å§‹ï¼ˆid ä¸º 0ï¼‰ç›´åˆ°å½“å‰èŠ‚ç‚¹
        """
        ancestors = [{"id": 0, "parent_id": 0, "name": ""}]
        if not id:
            return ancestors
        con: Any
        if isinstance(self, P115QueryDB):
            con = self.con
        else:
            con = self
        ls = list(query(con, """\
WITH t AS (
    SELECT id, parent_id, name FROM data WHERE id = ?
    UNION ALL
    SELECT data.id, data.parent_id, data.name FROM t JOIN data ON (t.parent_id = data.id)
)
SELECT id, parent_id, name FROM t;""", id))
        if not ls:
            raise FileNotFoundError(errno.ENOENT, id)
        if ls[-1][1]:
            raise ValueError(f"dangling id: {id}")
        ancestors.extend(dict(zip(("id", "parent_id", "name"), record)) for record in reversed(ls))
        return ancestors

    def get_attr(
        self, 
        id: int = 0, 
        /, 
    ) -> dict:
        """è·å–æŸä¸ªæ–‡ä»¶æˆ–ç›®å½•çš„ä¿¡æ¯

        :param self: P115QueryDB å®ä¾‹æˆ–è€…æ•°æ®åº“è¿æ¥æˆ–æ¸¸æ ‡
        :param id: å½“å‰èŠ‚ç‚¹çš„ id

        :return: å½“å‰èŠ‚ç‚¹çš„ä¿¡æ¯å­—å…¸
        """
        if not id:
            return {
                "id": 0, "parent_id": 0, "name": "", "sha1": "", "size": 0, "pickcode": "", 
                "is_dir": 1, "is_alive": 1, "extra": None, "created_at": 0, "updated_at": 0, 
            }
        con: Any
        if isinstance(self, P115QueryDB):
            con = self.con
        else:
            con = self
        return find(
            con, 
            f"SELECT * FROM data WHERE id=? LIMIT 1", 
            id, 
            FileNotFoundError(errno.ENOENT, id), 
            row_factory="dict", 
        )

    def get_id(
        self, 
        /, 
        pickcode: str = "", 
        sha1: str = "", 
        path: str = "", 
        is_alive: bool = True, 
    ) -> int:
        """æŸ¥è¯¢åŒ¹é…æŸä¸ªå­—æ®µçš„æ–‡ä»¶æˆ–ç›®å½•çš„ id

        :param self: P115QueryDB å®ä¾‹æˆ–è€…æ•°æ®åº“è¿æ¥æˆ–æ¸¸æ ‡
        :param pickcode: å½“å‰èŠ‚ç‚¹çš„æå–ç ï¼Œä¼˜å…ˆçº§é«˜äº sha1
        :param sha1: å½“å‰èŠ‚ç‚¹çš„ sha1 æ ¡éªŒæ•£åˆ—å€¼ï¼Œä¼˜å…ˆçº§é«˜äº path
        :param path: å½“å‰èŠ‚ç‚¹çš„è·¯å¾„

        :return: å½“å‰èŠ‚ç‚¹çš„ id
        """
        con: Any
        if isinstance(self, P115QueryDB):
            con = self.con
        else:
            con = self
        insertion = " AND is_alive" if is_alive else ""
        if pickcode:
            return find(
                con, 
                f"SELECT id FROM data WHERE pickcode=?{insertion} LIMIT 1", 
                pickcode, 
                default=FileNotFoundError(pickcode), 
            )
        elif sha1:
            return find(
                con, 
                f"SELECT id FROM data WHERE sha1=?{insertion} LIMIT 1", 
                sha1, 
                default=FileNotFoundError(sha1), 
            )
        elif path:
            return P115QueryDB.id_to_path(con, path)
        return 0

    def get_parent_id(
        self, 
        id: int = 0, 
        /, 
        default: None | int = None, 
    ) -> int:
        """è·å–æŸä¸ª id å¯¹åº”çš„çˆ¶ id

        :param self: P115QueryDB å®ä¾‹æˆ–è€…æ•°æ®åº“è¿æ¥æˆ–æ¸¸æ ‡
        :param id: å½“å‰èŠ‚ç‚¹çš„ id

        :return: å½“å‰èŠ‚ç‚¹çš„çˆ¶ id
        """
        if id == 0:
            return 0
        con: Any
        if isinstance(self, P115QueryDB):
            con = self.con
        else:
            con = self
        sql = "SELECT parent_id FROM data WHERE id=?"
        return find(con, sql, id, FileNotFoundError(errno.ENOENT, id) if default is None else default)

    def get_path(
        self, 
        id: int = 0, 
        /, 
    ) -> str:
        """è·å–æŸä¸ªæ–‡ä»¶æˆ–ç›®å½•çš„è·¯å¾„

        :param self: P115QueryDB å®ä¾‹æˆ–è€…æ•°æ®åº“è¿æ¥æˆ–æ¸¸æ ‡
        :param id: å½“å‰èŠ‚ç‚¹çš„ id

        :return: å½“å‰èŠ‚ç‚¹çš„è·¯å¾„
        """
        if not id:
            return "/"
        ancestors = P115QueryDB.get_ancestors(self, id)
        return "/".join(escape(a["name"]) for a in ancestors)

    def get_pickcode(
        self, 
        /, 
        id: int = -1, 
        sha1: str = "", 
        path: str = "", 
        is_alive: bool = True, 
    ) -> str:
        """æŸ¥è¯¢åŒ¹é…æŸä¸ªå­—æ®µçš„æ–‡ä»¶æˆ–ç›®å½•çš„æå–ç 

        :param self: P115QueryDB å®ä¾‹æˆ–è€…æ•°æ®åº“è¿æ¥æˆ–æ¸¸æ ‡
        :param id: å½“å‰èŠ‚ç‚¹çš„ idï¼Œä¼˜å…ˆçº§é«˜äº sha1
        :param sha1: å½“å‰èŠ‚ç‚¹çš„ sha1 æ ¡éªŒæ•£åˆ—å€¼ï¼Œä¼˜å…ˆçº§é«˜äº path
        :param path: å½“å‰èŠ‚ç‚¹çš„è·¯å¾„

        :return: å½“å‰èŠ‚ç‚¹çš„æå–ç 
        """
        con: Any
        if isinstance(self, P115QueryDB):
            con = self.con
        else:
            con = self
        insertion = " AND is_alive" if is_alive else ""
        if id >= 0:
            if not id:
                return ""
            return find(
                con, 
                f"SELECT pickcode FROM data WHERE id=?{insertion} LIMIT 1;", 
                id, 
                default=FileNotFoundError(id), 
            )
        elif sha1:
            return find(
                con, 
                f"SELECT pickcode FROM data WHERE sha1=?{insertion} LIMIT 1;", 
                sha1, 
                default=FileNotFoundError(sha1), 
            )
        else:
            if path in ("", "/"):
                return ""
            return P115QueryDB.get_pickcode(con, P115QueryDB.id_to_path(con, path))

    def get_sha1(
        self, 
        /, 
        id: int = -1, 
        pickcode: str = "", 
        path: str = "", 
        is_alive: bool = True, 
    ) -> str:
        """æŸ¥è¯¢åŒ¹é…æŸä¸ªå­—æ®µçš„æ–‡ä»¶çš„ sha1

        :param self: P115QueryDB å®ä¾‹æˆ–è€…æ•°æ®åº“è¿æ¥æˆ–æ¸¸æ ‡
        :param id: å½“å‰èŠ‚ç‚¹çš„ idï¼Œä¼˜å…ˆçº§é«˜äº pickcode
        :param pickcode: å½“å‰èŠ‚ç‚¹çš„æå–ç ï¼Œä¼˜å…ˆçº§é«˜äº path
        :param path: å½“å‰èŠ‚ç‚¹çš„è·¯å¾„

        :return: å½“å‰èŠ‚ç‚¹çš„ sha1 æ ¡éªŒæ•£åˆ—å€¼
        """
        con: Any
        if isinstance(self, P115QueryDB):
            con = self.con
        else:
            con = self
        insertion = " AND is_alive" if is_alive else ""
        if id >= 0:
            if not id:
                return ""
            return find(
                con, 
                f"SELECT sha1 FROM data WHERE id=?{insertion} LIMIT 1;", 
                id, 
                default=FileNotFoundError(id), 
            )
        elif pickcode:
            return find(
                con, 
                f"SELECT sha1 FROM data WHERE pickcode=?{insertion} LIMIT 1;", 
                pickcode, 
                default=FileNotFoundError(pickcode), 
            )
        else:
            if path in ("", "/"):
                return ""
            return P115QueryDB.get_sha1(con, P115QueryDB.id_to_path(con, path))

    def has_id(
        self, 
        id: int, 
        /, 
        is_alive: bool = True, 
    ) -> int:
        """æ˜¯å¦å­˜åœ¨æŸä¸ª id

        :param self: P115QueryDB å®ä¾‹æˆ–è€…æ•°æ®åº“è¿æ¥æˆ–æ¸¸æ ‡
        :param id: å½“å‰èŠ‚ç‚¹çš„ id
        :param is_alive: æ˜¯å¦å­˜æ´»

        :return: å¦‚æœæ˜¯ 1ï¼Œåˆ™æ˜¯ Trueï¼›å¦‚æœæ˜¯ 0ï¼Œåˆ™æ˜¯ False
        """
        if id == 0:
            return 1
        elif id < 0:
            return 0
        con: Any
        if isinstance(self, P115QueryDB):
            con = self.con
        else:
            con = self
        sql = "SELECT 1 FROM data WHERE id=?"
        if is_alive:
            sql += " AND is_alive"
        return find(con, sql, id, 0)

    def id_to_path(
        self, 
        /, 
        path: str | Sequence[str] = "", 
        ensure_file: None | bool = None, 
        parent_id: int = 0, 
    ) -> int:
        """æŸ¥è¯¢åŒ¹é…æŸä¸ªè·¯å¾„çš„æ–‡ä»¶æˆ–ç›®å½•çš„ä¿¡æ¯å­—å…¸ï¼Œåªè¿”å›æ‰¾åˆ°çš„ç¬¬ 1 ä¸ª

        :param self: P115QueryDB å®ä¾‹æˆ–è€…æ•°æ®åº“è¿æ¥æˆ–æ¸¸æ ‡
        :param path: è·¯å¾„
        :param ensure_file: æ˜¯å¦æ–‡ä»¶

            - å¦‚æœä¸º Trueï¼Œå¿…é¡»æ˜¯æ–‡ä»¶
            - å¦‚æœä¸º Falseï¼Œå¿…é¡»æ˜¯ç›®å½•
            - å¦‚æœä¸º Noneï¼Œå¯ä»¥æ˜¯æ–‡ä»¶æˆ–ç›®å½•

        :param parent_id: é¡¶å±‚ç›®å½•çš„ id

        :return: æ‰¾åˆ°çš„ç¬¬ 1 ä¸ªåŒ¹é…çš„èŠ‚ç‚¹ id
        """
        try:
            return next(P115QueryDB.iter_id_to_path(self, path, ensure_file, parent_id))
        except StopIteration:
            raise FileNotFoundError(errno.ENOENT, path) from None

    def iter_count_dir(
        self, 
        parent_id: int = 0, 
        /, 
    ) -> Iterator[dict]:
        """è¿­ä»£è·å–æ‰€æœ‰æŒ‡å®š id ä¸‹æ‰€æœ‰ç›®å½•èŠ‚ç‚¹ï¼ˆåŒ…æ‹¬è‡ªå·±ï¼‰ç›´å±çš„æ–‡ä»¶æ•°å’Œç›®å½•æ•°

        :param self: P115QueryDB å®ä¾‹æˆ–è€…æ•°æ®åº“è¿æ¥æˆ–æ¸¸æ ‡
        :param parent_id: é¡¶å±‚ç›®å½•çš„ id

        :return: è¿­ä»£å™¨ï¼Œè¿”å›å­—å…¸

            .. code::

                {
                    "id": int, 
                    "parent_id": int, 
                    "dir_count": int, 
                    "file_count": int, 
                }
        """
        con: Any
        if isinstance(self, P115QueryDB):
            con = self.con
        else:
            con = self
        sql = """\
WITH t AS (
    SELECT id, parent_id, is_dir FROM data WHERE parent_id = ? AND is_alive
    UNION ALL
    SELECT data.id, data.parent_id, data.is_dir FROM t JOIN data ON (t.id = data.parent_id) WHERE is_alive
), count AS (
    SELECT parent_id AS id, SUM(is_dir) AS dir_count, SUM(NOT is_dir) AS file_count 
    FROM t 
    GROUP BY parent_id
)
SELECT data.parent_id, count.* FROM count JOIN data USING (id)"""
        return query(con, sql, parent_id, row_factory="dict")

    def iter_count_tree(
        self, 
        parent_id: int = 0, 
        /, 
    ) -> Iterator[dict]:
        """è¿­ä»£è·å–æ‰€æœ‰æŒ‡å®š id ä¸‹æ‰€æœ‰ç›®å½•èŠ‚ç‚¹ï¼ˆåŒ…æ‹¬è‡ªå·±ï¼‰ç›´å±çš„æ–‡ä»¶æ•°å’Œç›®å½•æ•°ï¼Œä»¥åŠå­æ ‘ä¸‹çš„æ–‡ä»¶æ•°åˆè®¡å’Œç›®å½•ğŸŒ²åˆè®¡

        :param self: P115QueryDB å®ä¾‹æˆ–è€…æ•°æ®åº“è¿æ¥æˆ–æ¸¸æ ‡
        :param parent_id: é¡¶å±‚ç›®å½•çš„ id

        :return: è¿­ä»£å™¨ï¼Œè¿”å›å­—å…¸

            .. code::

                {
                    "id": int, 
                    "parent_id": int, 
                    "dir_count": int, 
                    "file_count": int, 
                    "tree_dir_count": int, 
                    "tree_file_count": int, 
                }
        """
        from iter_collect import grouped_mapping
        data = {a["id"]: a for a in P115QueryDB.iter_count_dir(self, parent_id)}
        id_to_children = grouped_mapping((a["parent_id"], id) for id, a in data.items())
        def calc(attr: dict, /) -> dict:
            if children := id_to_children.get(attr["id"]):
                for cid in children:
                    cattr = data[cid]
                    if "tree_dir_count" not in cattr:
                        calc(cattr)
                    attr["tree_dir_count"] = attr.get("tree_dir_count", 0) + 1 + cattr["tree_dir_count"]
                    attr["tree_file_count"] = attr.get("tree_file_count", attr["file_count"]) + cattr["tree_file_count"]
            elif "tree_dir_count" not in attr:
                attr["tree_dir_count"] = attr["dir_count"]
                attr["tree_file_count"] = attr["file_count"]
            return attr
        return map(calc, data.values())

    def iter_children(
        self, 
        parent_id: int | dict = 0, 
        /, 
        ensure_file: None | bool = None, 
    ) -> Iterator[dict]:
        """è·å–æŸä¸ªç›®å½•ä¹‹ä¸‹çš„æ–‡ä»¶æˆ–ç›®å½•çš„ä¿¡æ¯

        :param self: P115QueryDB å®ä¾‹æˆ–è€…æ•°æ®åº“è¿æ¥æˆ–æ¸¸æ ‡
        :param parent_id: çˆ¶ç›®å½•çš„ id
        :param ensure_file: æ˜¯å¦ä»…è¾“å‡ºæ–‡ä»¶

            - å¦‚æœä¸º Trueï¼Œä»…è¾“å‡ºæ–‡ä»¶
            - å¦‚æœä¸º Falseï¼Œä»…è¾“å‡ºç›®å½•
            - å¦‚æœä¸º Noneï¼Œå…¨éƒ¨è¾“å‡º

        :return: è¿­ä»£å™¨ï¼Œäº§ç”Ÿä¸€ç»„ä¿¡æ¯çš„å­—å…¸
        """
        con: Any
        if isinstance(self, P115QueryDB):
            con = self.con
        else:
            con = self
        if isinstance(parent_id, int):
            attr = P115QueryDB.get_attr(con, parent_id)
        else:
            attr = parent_id
        if not attr["is_dir"]:
            raise NotADirectoryError(errno.ENOTDIR, attr)
        sql = "SELECT * FROM data WHERE parent_id=? AND is_alive"
        if ensure_file is not None:
            if ensure_file:
                sql += " AND NOT is_dir"
            else:
                sql += " AND is_dir"
        return query(con, sql, attr["id"], row_factory="dict")

    def iter_dangling_ids(
        self, 
        /, 
    ) -> Iterator[int]:
        """ç½—åˆ—æ‰€æœ‰æ‚¬ç©ºçš„æ–‡ä»¶æˆ–ç›®å½•çš„ id

        .. note::
            æ‚¬ç©ºçš„ idï¼Œå³ç¥–å…ˆèŠ‚ç‚¹ä¸­ï¼Œå­˜åœ¨ä¸€ä¸ªèŠ‚ç‚¹ï¼Œå®ƒçš„ parent_id æ˜¯æ‚¬ç©ºçš„

        :param self: P115QueryDB å®ä¾‹æˆ–è€…æ•°æ®åº“è¿æ¥æˆ–æ¸¸æ ‡

        :return: è¿­ä»£å™¨ï¼Œä¸€ç»„ç›®å½•çš„ id
        """
        con: Any
        if isinstance(self, P115QueryDB):
            con = self.con
        else:
            con = self
        sql = """\
WITH dangling_ids(id) AS (
    SELECT d1.id
    FROM data AS d1 LEFT JOIN data AS d2 ON (d1.parent_id = d2.id)
    WHERE d1.parent_id AND d2.id IS NULL
    UNION ALL
    SELECT data.id FROM dangling_ids JOIN data ON (dangling_ids.id = data.parent_id)
)
SELECT id FROM dangling_ids"""
        return query(con, sql, row_factory="one")

    def iter_dangling_parent_ids(
        self, 
        /, 
    ) -> Iterator[int]:
        """ç½—åˆ—æ‰€æœ‰æ‚¬ç©ºçš„ parent_id

        .. note::
            æ‚¬ç©ºçš„ parent_idï¼Œå³æ‰€æœ‰çš„ parent_id ä¸­ï¼Œï¼Œä¸ä¸º 0 ä¸”ä¸åœ¨ `data` è¡¨ä¸­çš„éƒ¨åˆ†

        :param self: P115QueryDB å®ä¾‹æˆ–è€…æ•°æ®åº“è¿æ¥æˆ–æ¸¸æ ‡

        :return: è¿­ä»£å™¨ï¼Œä¸€ç»„ç›®å½•çš„ id
        """
        con: Any
        if isinstance(self, P115QueryDB):
            con = self.con
        else:
            con = self
        sql = """\
SELECT
    DISTINCT d1.parent_id
FROM
    data AS d1 LEFT JOIN data AS d2 ON (d1.parent_id = d2.id)
WHERE
    d1.parent_id AND d2.id IS NULL"""
        return query(con, sql, row_factory="one")

    def iter_descendants(
        self, 
        parent_id: int | dict = 0, 
        /, 
        min_depth: int = 1, 
        max_depth: int = -1, 
        ensure_file: None | bool = None, 
        use_relpath: None | bool = False, 
        with_root: bool = False, 
        topdown: None | bool = True, 
    ) -> Iterator[dict]:
        """éå†è·å–æŸä¸ªç›®å½•ä¹‹ä¸‹çš„æ‰€æœ‰æ–‡ä»¶æˆ–ç›®å½•çš„ä¿¡æ¯

        :param self: P115QueryDB å®ä¾‹æˆ–è€…æ•°æ®åº“è¿æ¥æˆ–æ¸¸æ ‡
        :param parent_id: é¡¶å±‚ç›®å½•çš„ id
        :param min_depth: æœ€å°æ·±åº¦
        :param max_depth: æœ€å¤§æ·±åº¦ã€‚å¦‚æœå°äº 0ï¼Œåˆ™æ— é™æ·±åº¦
        :param ensure_file: æ˜¯å¦ä»…è¾“å‡ºæ–‡ä»¶

            - å¦‚æœä¸º Trueï¼Œä»…è¾“å‡ºæ–‡ä»¶
            - å¦‚æœä¸º Falseï¼Œä»…è¾“å‡ºç›®å½•
            - å¦‚æœä¸º Noneï¼Œå…¨éƒ¨è¾“å‡º

        :param use_relpath: æ˜¯å¦ä»…è¾“å‡ºç›¸å¯¹è·¯å¾„ã€‚å¦‚æœä¸º Falseï¼Œåˆ™è¾“å‡ºå®Œæ•´è·¯å¾„ï¼ˆä» / å¼€å§‹ï¼‰ï¼›å¦‚æœä¸º Noneï¼Œåˆ™ä¸è¾“å‡º "ancestors", "path", "posixpath"
        :param with_root: ä»…å½“ `use_relpath=True` æ—¶ç”Ÿæ•ˆã€‚å¦‚æœä¸º Trueï¼Œåˆ™ç›¸å¯¹è·¯å¾„åŒ…å« `parent_id` å¯¹åº”çš„èŠ‚ç‚¹
        :param topdown: æ˜¯å¦è‡ªé¡¶å‘ä¸‹æ·±åº¦ä¼˜å…ˆéå†

            - å¦‚æœä¸º Trueï¼Œåˆ™è‡ªé¡¶å‘ä¸‹æ·±åº¦ä¼˜å…ˆéå†
            - å¦‚æœä¸º Falseï¼Œåˆ™è‡ªåº•å‘ä¸Šæ·±åº¦ä¼˜å…ˆéå†
            - å¦‚æœä¸º Noneï¼Œåˆ™è‡ªé¡¶å‘ä¸‹å®½åº¦ä¼˜å…ˆéå†

        :return: è¿­ä»£å™¨ï¼Œäº§ç”Ÿä¸€ç»„ä¿¡æ¯çš„å­—å…¸ï¼ŒåŒ…å«å¦‚ä¸‹å­—æ®µï¼š

            .. code:: python

                (
                    "id", "parent_id", "pickcode", "sha1", "name", "size", "is_dir", 
                    "type", "is_alive", "extra", "created_at", "updated_at", 
                    "depth", "ancestors", "path", "posixpath", 
                )
        """
        con: Any
        if isinstance(self, P115QueryDB):
            con = self.con
        else:
            con = self
        with_path = use_relpath is not None
        if isinstance(parent_id, int):
            if 0 <= max_depth < min_depth:
                return
            depth = 1
            if with_path:
                if not parent_id:
                    ancestors: list[dict] = [{"id": 0, "parent_id": 0, "name": ""}]
                    dir_ = posixdir = "/"
                elif use_relpath:
                    if with_root:
                        attr = parent_id = P115QueryDB.get_attr(con, parent_id)
                        name = attr["name"]
                        ancestors = [{"id": attr["id"], "parent_id": attr["parent_id"], "name": name}]
                        dir_ = escape(name) + "/"
                        posixdir = name.replace("/", "|") + "/"
                    else:
                        ancestors = []
                        dir_ = posixdir = ""
                else:
                    ancestors = P115QueryDB.get_ancestors(con, parent_id)
                    dir_ = "/".join(escape(a["name"]) for a in ancestors) + "/"
                    posixdir = "/".join(a["name"].replace("/", "|") for a in ancestors) + "/"
        else:
            attr = parent_id
            depth = attr["depth"] + 1
            if with_path:
                ancestors = attr["ancestors"]
                dir_ = attr["path"]
                posixdir = attr["posixpath"]
                if dir_ != "/":
                    dir_ += "/"
                    posixdir += "/"
        if topdown is None:
            if with_path:
                gen = bfs_gen((parent_id, 0, ancestors, dir_, posixdir))
            else:
                gen = bfs_gen((parent_id, 0)) # type: ignore
            send: Callable = gen.send
            p: list
            for parent_id, depth, *p in gen:
                depth += 1
                will_step_in = max_depth < 0 or depth < max_depth
                will_yield = min_depth <= depth and (max_depth < 0 or depth <= max_depth)
                if with_path:
                    ancestors, dir_, posixdir = p
                for attr in P115QueryDB.iter_children(con, parent_id, False if ensure_file is False else None):
                    attr["depth"] = depth
                    is_dir = attr["is_dir"]
                    if with_path:
                        attr["ancestors"] = [
                            *ancestors, 
                            {k: attr[k] for k in ("id", "parent_id", "name")}, 
                        ]
                        attr["path"] = dir_ + escape(attr["name"])
                        attr["posixpath"] = posixdir + attr["name"].replace("/", "|")
                    if is_dir and will_step_in:
                        if with_path:
                            send((attr, depth, attr["ancestors"], attr["path"] + "/", attr["posixpath"] + "/"))
                        else:
                            send((attr, depth))
                    if will_yield:
                        if ensure_file is None:
                            yield attr
                        elif is_dir:
                            if not ensure_file:
                                yield attr
                        elif ensure_file:
                            yield attr
        else:
            will_step_in = max_depth < 0 or depth < max_depth
            will_yield = min_depth <= depth and (max_depth < 0 or depth <= max_depth)
            for attr in P115QueryDB.iter_children(con, parent_id, False if ensure_file is False else None):
                is_dir = attr["is_dir"]
                attr["depth"] = depth
                if with_path:
                    attr["ancestors"] = [
                        *ancestors, 
                        {k: attr[k] for k in ("id", "parent_id", "name")}, 
                    ]
                    attr["path"] = dir_ + escape(attr["name"])
                    attr["posixpath"] = posixdir + attr["name"].replace("/", "|")
                if will_yield and topdown:
                    if ensure_file is None:
                        yield attr
                    elif is_dir:
                        if not ensure_file:
                            yield attr
                    elif ensure_file:
                        yield attr
                if is_dir and will_step_in:
                    yield from P115QueryDB.iter_descendants(
                        con, 
                        attr, 
                        min_depth=min_depth, 
                        max_depth=max_depth, 
                        ensure_file=ensure_file, 
                        use_relpath=use_relpath, 
                        with_root=with_root, 
                        topdown=topdown, 
                    )
                if will_yield and not topdown:
                    if ensure_file is None:
                        yield attr
                    elif is_dir:
                        if not ensure_file:
                            yield attr
                    elif ensure_file:
                        yield attr

    @overload
    def iter_descendants_bfs(
        self, 
        parent_id: int = 0, 
        /, 
        min_depth: int = 1, 
        max_depth: int = -1, 
        ensure_file: None | bool = None, 
        use_relpath: bool = False, 
        with_root: bool = False, 
        *, 
        fields: str, 
    ) -> Iterator[Any]:
        ...
    @overload
    def iter_descendants_bfs(
        self, 
        parent_id: int = 0, 
        /, 
        min_depth: int = 1, 
        max_depth: int = -1, 
        ensure_file: None | bool = None, 
        use_relpath: bool = False, 
        with_root: bool = False, 
        *, 
        fields: tuple[str, ...] = (
            "id", "parent_id", "name", "sha1", "size", "pickcode", 
            "is_dir", "is_alive", "extra", "created_at", "updated_at", 
            "depth", "ancestors", "path", "posixpath", 
        ), 
        to_dict: Literal[False], 
    ) -> Iterator[tuple[Any, ...]]:
        ...
    @overload
    def iter_descendants_bfs(
        self, 
        parent_id: int = 0, 
        /, 
        min_depth: int = 1, 
        max_depth: int = -1, 
        ensure_file: None | bool = None, 
        use_relpath: bool = False, 
        with_root: bool = False, 
        *, 
        fields: tuple[str, ...] = (
            "id", "parent_id", "name", "sha1", "size", "pickcode", 
            "is_dir", "is_alive", "extra", "created_at", "updated_at", 
            "depth", "ancestors", "path", "posixpath", 
        ), 
        to_dict: Literal[True] = True, 
    ) -> Iterator[dict[str, Any]]:
        ...
    def iter_descendants_bfs(
        self, 
        parent_id: int = 0, 
        /, 
        min_depth: int = 1, 
        max_depth: int = -1, 
        ensure_file: None | bool = None, 
        use_relpath: bool = False, 
        with_root: bool = False, 
        *, 
        fields: str | tuple[str, ...] = (
            "id", "parent_id", "name", "sha1", "size", "pickcode", 
            "is_dir", "is_alive", "extra", "created_at", "updated_at", 
            "depth", "ancestors", "path", "posixpath", 
        ), 
        to_dict: bool = True, 
    ) -> Iterator:
        """è·å–æŸä¸ªç›®å½•ä¹‹ä¸‹çš„æ‰€æœ‰ç›®å½•èŠ‚ç‚¹çš„ id æˆ–è€…ä¿¡æ¯å­—å…¸ï¼ˆå®½åº¦ä¼˜å…ˆéå†ï¼‰

        :param self: P115QueryDB å®ä¾‹æˆ–è€…æ•°æ®åº“è¿æ¥æˆ–æ¸¸æ ‡
        :param parent_id: é¡¶å±‚ç›®å½•çš„ id
        :param min_depth: æœ€å°æ·±åº¦
        :param max_depth: æœ€å¤§æ·±åº¦ã€‚å¦‚æœå°äº 0ï¼Œåˆ™æ— é™æ·±åº¦
        :param ensure_file: æ˜¯å¦ä»…è¾“å‡ºæ–‡ä»¶

            - å¦‚æœä¸º Trueï¼Œä»…è¾“å‡ºæ–‡ä»¶
            - å¦‚æœä¸º Falseï¼Œä»…è¾“å‡ºç›®å½•
            - å¦‚æœä¸º Noneï¼Œå…¨éƒ¨è¾“å‡º

        :param use_relpath: ä»…è¾“å‡ºç›¸å¯¹è·¯å¾„ï¼Œå¦åˆ™è¾“å‡ºå®Œæ•´è·¯å¾„ï¼ˆä» / å¼€å§‹ï¼‰
        :param with_root: ä»…å½“ `use_relpath=True` æ—¶ç”Ÿæ•ˆã€‚å¦‚æœä¸º Trueï¼Œåˆ™ç›¸å¯¹è·¯å¾„åŒ…å« `parent_id` å¯¹åº”çš„èŠ‚ç‚¹
        :param fields: éœ€è¦è·å–çš„å­—æ®µ

            - å¦‚æœä¸º strï¼Œåˆ™è·å–æŒ‡å®šçš„å­—æ®µçš„å€¼
            - å¦‚æœä¸º tupleï¼Œåˆ™æ‹‰å–è¿™ä¸€ç»„å­—æ®µçš„å€¼

        :param to_dict: æ˜¯å¦äº§ç”Ÿå­—å…¸ï¼Œå¦‚æœä¸º True ä¸” fields ä¸ä¸º strï¼Œåˆ™äº§ç”Ÿå­—å…¸

        :return: è¿­ä»£å™¨ï¼Œäº§ç”Ÿä¸€ç»„æ•°æ®
        """
        con: Any
        if isinstance(self, P115QueryDB):
            con = self.con
        else:
            con = self
        extended_fields = frozenset(("depth", "ancestors", "path", "posixpath"))
        one_value = False
        parse: None | Callable = None
        if isinstance(fields, str):
            one_value = True
            field = fields
            with_depth = max_depth > 1 or "depth" == field
            with_ancestors = "ancestors" == field
            with_path = "path" == field
            with_posixpath = "posixpath" == field
            fields = field,
        else:
            with_depth = max_depth > 1 or min_depth > 1 or "depth" in fields
            with_ancestors = "ancestors" in fields
            with_path = "path" in fields
            with_posixpath = "posixpath" in fields
        select_fields1 = ["id"]
        select_fields1.extend(set(fields) - {"id"} - extended_fields)
        select_fields2 = ["data." + f for f in select_fields1]
        where1, where2 = "", ""
        if with_depth:
            select_fields1.append("1 AS depth")
            select_fields2.append("t.depth + 1")
        if with_path or with_posixpath or with_ancestors:
            if not parent_id:
                ancestors: list[dict] = [{"id": 0, "parent_id": 0, "name": ""}]
                path = posixpath = "/"
            elif use_relpath:
                if with_root:
                    attr = P115QueryDB.get_attr(con, parent_id)
                    name = attr["name"]
                    ancestors = [{"id": attr["id"], "parent_id": attr["parent_id"], "name": name}]
                    path = escape(name) + "/"
                    posixpath = name.replace("/", "|") + "/"
                else:
                    ancestors = []
                    path = posixpath = ""
            else:
                ancestors = P115QueryDB.get_ancestors(con, parent_id)
                if with_path:
                    path = "/".join(escape(a["name"]) for a in ancestors) + "/"
                if with_posixpath:
                    posixpath = "/".join(a["name"].replace("/", "|") for a in ancestors) + "/"
            if with_ancestors:
                if ancestors:
                    parse_ancestors = lambda val: [*ancestors, *loads("[%s]" % val)]
                else:
                    parse_ancestors = lambda val: loads("[%s]" % val)
                parse = parse_ancestors
                select_fields1.append("json_object('id', id, 'parent_id', parent_id, 'name', name) AS ancestors")
                select_fields2.append("concat(t.ancestors, ',', json_object('id', data.id, 'parent_id', data.parent_id, 'name', data.name))")
            if with_path:
                def parse_path(val: str, /) -> str:
                    return path + val
                parse = parse_path
                if isinstance(con, Cursor):
                    conn = con.connection
                else:
                    conn = con
                conn.create_function("escape_name", 1, escape, deterministic=True)
                select_fields1.append("escape_name(name) AS path")
                select_fields2.append("concat(t.path, '/', escape_name(data.name))")
            if with_posixpath:
                def parse_posixpath(val: str, /) -> str:
                    return posixpath + val
                parse = parse_posixpath
                select_fields1.append("replace(name, '/', '|') AS posixpath")
                select_fields2.append("concat(t.posixpath, '/', replace(data.name, '/', '|'))")
        if min_depth <= 1 and max_depth in (0, 1) or 0 <= max_depth < min_depth:
            if 0 <= max_depth < min_depth:
                where1 = " AND FALSE"
            elif ensure_file:
                where1 = " AND NOT is_dir"
            elif ensure_file is False:
                where1 = " AND is_dir"
            sql = f"""\
    WITH t AS (
        SELECT {",".join(select_fields1)} FROM data WHERE parent_id={parent_id:d} AND is_alive{where1}
    ) SELECT {",".join(fields)} FROM t"""
        else:
            if max_depth > 1:
                where2 = f" AND depth < {max_depth:d}"
            if ensure_file:
                if "is_dir" not in fields:
                    select_fields1.append("is_dir")
                    select_fields2.append("data.is_dir")
            elif ensure_file is False:
                where1 += " AND is_dir"
                where2 += " AND data.is_dir"
            sql = f"""\
    WITH t AS (
        SELECT {",".join(select_fields1)} FROM data WHERE parent_id={parent_id:d} AND is_alive{where1}
        UNION ALL
        SELECT {",".join(select_fields2)} FROM t JOIN data ON(t.id = data.parent_id) WHERE data.is_alive{where2}
    ) SELECT {",".join(fields)} FROM t WHERE True"""
            if ensure_file:
                sql += " AND NOT is_dir"
            if min_depth > 1:
                sql += f" AND depth >= {min_depth:d}"
        if one_value:
            if parse is None:
                row_factory = lambda _, r: r[0]
            else:
                row_factory = lambda _, r: parse(r[0])
        elif to_dict:
            def row_factory(_, r):
                d = dict(zip(fields, r))
                if with_ancestors:
                    d["ancestors"] = parse_ancestors(d["ancestors"])
                if with_path:
                    d["path"] = parse_path(d["path"])
                if with_posixpath:
                    d["posixpath"] = parse_posixpath(d["posixpath"])
                return d
        else:
            with_route = with_ancestors or with_path or with_posixpath
            def parse(f, v):
                match f:
                    case "ancestors":
                        return parse_ancestors(v)
                    case "path":
                        return parse_path(v)
                    case "posixpath":
                        return parse_posixpath(v)
                    case _:
                        return v
            def row_factory(_, r):
                if with_route:
                    return tuple(parse(f, v) for f, v in zip(fields, r))
                return r
        return query(con, sql, row_factory=row_factory)

    def iter_dup_files(
        self, 
        /, 
    ) -> Iterator[dict]:
        """ç½—åˆ—æ‰€æœ‰é‡å¤æ–‡ä»¶

        :param self: P115QueryDB å®ä¾‹æˆ–è€…æ•°æ®åº“è¿æ¥æˆ–æ¸¸æ ‡

        :return: è¿­ä»£å™¨ï¼Œä¸€ç»„æ–‡ä»¶çš„ä¿¡æ¯
        """
        con: Any
        if isinstance(self, P115QueryDB):
            con = self.con
        else:
            con = self
        sql = f"""\
WITH stats AS (
    SELECT
        COUNT(1) OVER w AS total, 
        ROW_NUMBER() OVER w AS nth, 
        *
    FROM data
    WHERE NOT is_dir AND is_alive
    WINDOW w AS (PARTITION BY sha1, size)
)
SELECT * FROM stats WHERE total > 1"""
        return query(con, sql, row_factory="dict")

    def iter_existing_id(
        self, 
        ids: Iterable[int], 
        /, 
        is_alive: bool = True, 
    ) -> Iterator[int]:
        """ç­›é€‰å‡ºä¸€ç³»åˆ— id ä¸­ï¼Œåœ¨æ•°æ®åº“ä¸­å­˜åœ¨çš„

        :param self: P115QueryDB å®ä¾‹æˆ–è€…æ•°æ®åº“è¿æ¥æˆ–æ¸¸æ ‡
        :param ids: ä¸€ç³»åˆ—çš„èŠ‚ç‚¹ id
        :param is_alive: æ˜¯å¦å­˜æ´»

        :return: ä¸€ç³»åˆ— id ä¸­åœ¨æ•°æ®åº“ä¸­å­˜åœ¨çš„é‚£äº›çš„è¿­ä»£å™¨ï¼ˆå®é™…ç›´æ¥è¿”å›ä¸€ä¸ªæ¸¸æ ‡ï¼‰
        """
        con: Any
        if isinstance(self, P115QueryDB):
            con = self.con
        else:
            con = self
        sql = "SELECT id FROM data WHERE id IN (%s)" % (",".join(map("%d".__mod__, ids)) or "NULL")
        if is_alive:
            sql += " AND is_alive"
        return query(con, sql, row_factory="one")

    def iter_files_with_path_url(
        self, 
        parent_id: int = 0, 
        /, 
        base_url: str = "http://localhost:8000", 
    ) -> Iterator[tuple[str, str]]:
        """è¿­ä»£è·å–æ‰€æœ‰æ–‡ä»¶çš„è·¯å¾„å’Œä¸‹è½½é“¾æ¥

        :param self: P115QueryDB å®ä¾‹æˆ–è€…æ•°æ®åº“è¿æ¥æˆ–æ¸¸æ ‡
        :param parent_id: é¡¶å±‚ç›®å½•çš„ id
        :param base_url: 115 çš„ 302 æœåŠ¡åç«¯åœ°å€

        :return: è¿­ä»£å™¨ï¼Œè¿”å›æ¯ä¸ªæ–‡ä»¶çš„ è·¯å¾„ å’Œ ä¸‹è½½é“¾æ¥ çš„ 2 å…ƒç»„
        """
        from encode_uri import encode_uri_component_loose
        con: Any
        if isinstance(self, P115QueryDB):
            con = self.con
        else:
            con = self
        code = compile('f"%s/{quote(name, '"''"')}?{id=}&{pickcode=!s}&{sha1=!s}&{size=}&file=true"' % base_url.translate({ord(c): c*2 for c in "{}"}), "-", "eval")
        for attr in P115QueryDB.iter_descendants_bfs(
            con, 
            parent_id, 
            fields=("id", "sha1", "pickcode", "size", "name", "posixpath"), 
            ensure_file=True, 
        ):
            yield attr["posixpath"], eval(code, {"quote": encode_uri_component_loose}, attr)

    def iter_id_to_parent_id(
        self, 
        ids: Iterable[int], 
        /, 
        recursive: bool = False, 
    ) -> Iterator[tuple[int, int]]:
        """æ‰¾å‡ºä¸€ç³»åˆ— id æ‰€å¯¹åº”çš„çˆ¶ idï¼Œè¿”å› ``(id, parent_id)`` çš„ 2 å…ƒç»„

        :param self: P115QueryDB å®ä¾‹æˆ–è€…æ•°æ®åº“è¿æ¥æˆ–æ¸¸æ ‡
        :param ids: ä¸€ç³»åˆ—çš„èŠ‚ç‚¹ id
        :param recursive: æ˜¯å¦é€’å½’ï¼Œå¦‚æœä¸º Trueï¼Œåˆ™è¿˜ä¼šå¤„ç†å®ƒä»¬çš„ç¥–å…ˆèŠ‚ç‚¹

        :return: è¿­ä»£å™¨ï¼Œäº§ç”Ÿ ``(id, parent_id)`` çš„ 2 å…ƒç»„ï¼ˆå®é™…ç›´æ¥è¿”å›ä¸€ä¸ªæ¸¸æ ‡ï¼‰
        """
        con: Any
        if isinstance(self, P115QueryDB):
            con = self.con
        else:
            con = self
        s_ids = "(%s)" % (",".join(map(str, ids)) or "NULL")
        if recursive:
            sql = """\
WITH pairs AS (
    SELECT id, parent_id FROM data WHERE id IN %s
    UNION ALL
    SELECT data.id, data.parent_id FROM pairs JOIN data ON (pairs.parent_id = data.id)
) SELECT * FROM pairs""" % s_ids
        else:
            sql = "SELECT id, parent_id FROM data WHERE id IN %s" % s_ids
        return query(con, sql)

    def iter_id_to_path(
        self, 
        /, 
        path: str | Sequence[str] = "", 
        ensure_file: None | bool = None, 
        parent_id: int = 0, 
    ) -> Iterator[int]:
        """æŸ¥è¯¢åŒ¹é…æŸä¸ªè·¯å¾„çš„æ–‡ä»¶æˆ–ç›®å½•çš„ä¿¡æ¯å­—å…¸

        .. note::
            åŒä¸€ä¸ªè·¯å¾„å¯ä»¥æœ‰å¤šæ¡å¯¹åº”çš„æ•°æ®

        :param self: P115QueryDB å®ä¾‹æˆ–è€…æ•°æ®åº“è¿æ¥æˆ–æ¸¸æ ‡
        :param path: è·¯å¾„
        :param ensure_file: æ˜¯å¦æ–‡ä»¶

            - å¦‚æœä¸º Trueï¼Œå¿…é¡»æ˜¯æ–‡ä»¶
            - å¦‚æœä¸º Falseï¼Œå¿…é¡»æ˜¯ç›®å½•
            - å¦‚æœä¸º Noneï¼Œå¯ä»¥æ˜¯æ–‡ä»¶æˆ–ç›®å½•

        :param parent_id: é¡¶å±‚ç›®å½•çš„ id

        :return: è¿­ä»£å™¨ï¼Œäº§ç”Ÿä¸€ç»„åŒ¹é…æŒ‡å®šè·¯å¾„çš„ï¼ˆæ–‡ä»¶æˆ–ç›®å½•ï¼‰èŠ‚ç‚¹çš„ id
        """
        con: Any
        if isinstance(self, P115QueryDB):
            con = self.con
        else:
            con = self
        patht: Sequence[str]
        if isinstance(path, str):
            if ensure_file is None and path_is_dir_form(path):
                ensure_file = False
            patht, _ = splits("/" + path)
        else:
            patht = ("", *filter(None, path))
        if not parent_id and len(patht) == 1:
            return iter((0,))
        if len(patht) > 2:
            sql = "SELECT id FROM data WHERE parent_id=? AND name=? AND is_alive AND is_dir LIMIT 1"
            for name in patht[1:-1]:
                parent_id = find(con, sql, (parent_id, name), default=-1)
                if parent_id < 0:
                    return iter(())
        sql = "SELECT id FROM data WHERE parent_id=? AND name=? AND is_alive"
        if ensure_file is None:
            sql += " ORDER BY is_dir DESC"
        elif ensure_file:
            sql += " AND NOT is_dir"
        else:
            sql += " AND is_dir LIMIT 1"
        return query(con, sql, (parent_id, patht[-1]), row_factory="one")

    def iter_parent_id(
        self, 
        ids: Iterable[int], 
        /, 
    ) -> Iterator[int]:
        """æ‰¾å‡ºä¸€ç³»åˆ— id æ‰€å¯¹åº”çš„çˆ¶ id

        :param self: P115QueryDB å®ä¾‹æˆ–è€…æ•°æ®åº“è¿æ¥æˆ–æ¸¸æ ‡
        :param ids: ä¸€ç³»åˆ—çš„èŠ‚ç‚¹ id

        :return: å®ƒä»¬çš„çˆ¶ id çš„è¿­ä»£å™¨ï¼ˆå®é™…ç›´æ¥è¿”å›ä¸€ä¸ªæ¸¸æ ‡ï¼‰
        """
        con: Any
        if isinstance(self, P115QueryDB):
            con = self.con
        else:
            con = self
        sql = "SELECT parent_id FROM data WHERE id IN (%s)" % (",".join(map("%d".__mod__, ids)) or "NULL")
        return query(con, sql, row_factory="one")

    def select_na_ids(
        self, 
        /, 
    ) -> set[int]:
        """æ‰¾å‡ºæ‰€æœ‰çš„å¤±æ•ˆèŠ‚ç‚¹å’Œæ‚¬ç©ºèŠ‚ç‚¹çš„ id

        .. note::
            æ‚¬ç©ºèŠ‚ç‚¹ï¼Œå°±æ˜¯æ­¤èŠ‚ç‚¹æœ‰ä¸€ä¸ªç¥–å…ˆèŠ‚ç‚¹çš„ parant_idï¼Œä¸ä¸º 0 ä¸”ä¸åœ¨ `data` è¡¨ä¸­

        :param self: P115QueryDB å®ä¾‹æˆ–è€…æ•°æ®åº“è¿æ¥æˆ–æ¸¸æ ‡

        :return: ä¸€ç»„æ‚¬ç©ºèŠ‚ç‚¹çš„ id çš„é›†åˆ
        """
        con: Any
        if isinstance(self, P115QueryDB):
            con = self.con
        else:
            con = self
        ok_ids: set[int] = set(query(con, "SELECT id FROM data WHERE NOT is_alive", row_factory="one"))
        na_ids: set[int] = set()
        d = dict(query(con, "SELECT id, parent_id FROM data WHERE is_alive"))
        temp: list[int] = []
        push = temp.append
        clear = temp.clear
        update_ok = ok_ids.update
        update_na = na_ids.update
        for k in d:
            try:
                push(k)
                while k := d[k]:
                    if k in ok_ids:
                        update_ok(temp)
                        break
                    elif k in na_ids:
                        update_na(temp)
                        break
                    push(k)
                else:
                    update_ok(temp)
            except KeyError:
                update_na(temp)
            finally:
                clear()
        return na_ids

    def dump_to_alist(
        self, 
        /, 
        alist_db: str | PathLike | Connection | Cursor = expanduser("~/alist.d/data/data.db"), 
        parent_id: int = 0, 
        dirname: str = "/115", 
        clean: bool = True, 
    ) -> int:
        """æŠŠ p115updatedb å¯¼å‡ºçš„æ•°æ®ï¼Œå¯¼å…¥åˆ° alist çš„æœç´¢ç´¢å¼•

        :param self: P115QueryDB å®ä¾‹æˆ–è€…æ•°æ®åº“è¿æ¥æˆ–æ¸¸æ ‡
        :param alist_db: alist æ•°æ®åº“æ–‡ä»¶è·¯å¾„æˆ–è¿æ¥
        :param parent_id: åœ¨ p115updatedb æ‰€å¯¼å‡ºæ•°æ®åº“ä¸­çš„é¡¶å±‚ç›®å½• id
        :param dirname: åœ¨ alist ä¸­æ‰€å¯¹åº”çš„çš„é¡¶å±‚ç›®å½•è·¯å¾„
        :param clean: åœ¨æ’å…¥å‰å…ˆæ¸…é™¤ alist çš„æ•°æ®åº“ä¸­ `dirname` ç›®å½•ä¸‹çš„æ‰€æœ‰æ•°æ®

        :return: æ€»å…±å¯¼å…¥çš„æ•°é‡
        """
        con: Any
        if isinstance(self, P115QueryDB):
            con = self.con
        else:
            con = self
        sql = """\
WITH t AS (
    SELECT 
        :dirname AS parent, 
        name, 
        is_dir, 
        size, 
        id, 
        CASE WHEN is_dir THEN CONCAT(:dirname, '/', REPLACE(name, '/', '|')) END AS dirname 
    FROM data WHERE parent_id=:parent_id AND is_alive
    UNION ALL
    SELECT 
        t.dirname AS parent, 
        data.name, 
        data.is_dir, 
        data.size, 
        data.id, 
        CASE WHEN data.is_dir THEN CONCAT(t.dirname, '/', REPLACE(data.name, '/', '|')) END AS dirname
    FROM t JOIN data ON(t.id = data.parent_id) WHERE data.is_alive
)
SELECT parent, name, is_dir, size FROM t"""
        dirname = "/" + dirname.strip("/")
        with transact(alist_db) as cur:
            if clean:
                cur.execute("DELETE FROM x_search_nodes WHERE parent=? OR parent LIKE ? || '/%';", (dirname, dirname))
            count = 0
            executemany = cur.executemany
            for items in batched(query(con, sql, {"parent_id": parent_id, "dirname": dirname}), 10_000):
                executemany("INSERT INTO x_search_nodes(parent, name, is_dir, size) VALUES (?, ?, ?, ?)", items)
                count += len(items)
            return count

    def dump_efu(
        self, 
        /, 
        efu_file: str | PathLike = "export.efu", 
        parent_id: int = 0, 
        dirname: str = "", 
        use_relpath: bool = False, 
    ) -> int:
        """æŠŠ p115updatedb å¯¼å‡ºçš„æ•°æ®ï¼Œå¯¼å‡ºä¸º efu æ–‡ä»¶ï¼Œå¯ä¾› everything è½¯ä»¶ä½¿ç”¨

        :param self: P115QueryDB å®ä¾‹æˆ–è€…æ•°æ®åº“è¿æ¥æˆ–æ¸¸æ ‡
        :param efu_file: è¦å¯¼å‡ºçš„æ–‡ä»¶è·¯å¾„
        :param parent_id: åœ¨ p115updatedb æ‰€å¯¼å‡ºæ•°æ®åº“ä¸­çš„é¡¶å±‚ç›®å½• id
        :param dirname: ç»™æ¯ä¸ªå¯¼å‡ºè·¯å¾„æ·»åŠ çš„ç›®å½•å‰ç¼€
        :param use_relpath: æ˜¯å¦ä½¿ç”¨ç›¸å¯¹è·¯å¾„

        :return: æ€»å…±å¯¼å‡ºçš„æ•°é‡
        """
        con: Any
        if isinstance(self, P115QueryDB):
            con = self.con
        else:
            con = self
        def unix_to_filetime(unix_time: float, /) -> int:
            return int(unix_time * 10 ** 7) + 11644473600 * 10 ** 7
        if dirname:
            dirname = normpath(dirname)
            if not dirname.endswith("\\"):
                dirname += "\\"
        n = 0
        with open(efu_file, "w", newline="", encoding="utf-8") as file:
            csvfile = writer(file)
            writerow = csvfile.writerow
            writerow(("Filename", "Size", "Date Modified", "Date Created", "Attributes"))
            for n, (size, ctime, mtime, is_dir, path) in enumerate(P115QueryDB.iter_descendants_bfs(
                con, 
                parent_id, 
                use_relpath=use_relpath, 
                fields=("size", "created_at", "updated_at", "is_dir", "posixpath"), 
                to_dict=False, 
            ), 1):
                if use_relpath:
                    path = normpath(path)
                else:
                    path = normpath(path[1:])
                writerow((
                    dirname + path, 
                    size, 
                    unix_to_filetime(mtime), 
                    unix_to_filetime(ctime), 
                    16 if is_dir else 0, 
                ))
        return n

